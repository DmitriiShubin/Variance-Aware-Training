{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dmitriishubin/Desktop/physionet-challenge-2020\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### insert the name of the last run\n",
    "!tensorboard --logdir=runs/May11_01-21-42_dmitrii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from metrics import Metric\n",
    "from postprocessing import PostProcessing\n",
    "from utils.KPI_plots import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_waveform(data_path,name):\n",
    "    \n",
    "    signal = np.load(data_path+name+'.npy')\n",
    "        \n",
    "    return signal\n",
    "\n",
    "def load_label(data_path,name):\n",
    "    \n",
    "    label = json.load(open(data_path+name+'.json'))\n",
    "        \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "competition_metric = Metric()\n",
    "postprocessing = PostProcessing(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'E'\n",
    "\n",
    "DEBUG_PATH = f'./data/CV_debug/{dataset}/'\n",
    "DATA_PATH = f'./data/{dataset}/formatted/'\n",
    "\n",
    "list_records = [i[:-5] for i in os.listdir(DEBUG_PATH) if i.find('.json')!=-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitriishubin/Desktop/physionet-challenge-2020/utils/KPI_plots.py:54: RuntimeWarning: invalid value encountered in true_divide\n",
      "  cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n"
     ]
    }
   ],
   "source": [
    "# Main processing pipeline\n",
    "scores_errors = np.array([])\n",
    "scores_competition = np.array([])\n",
    "records = np.array([]).astype(np.str)\n",
    "labels = [] #np.array([])\n",
    "preds = [] #np.array([])\n",
    "\n",
    "#load all records\n",
    "for record in list_records:\n",
    "    \n",
    "    label = load_label(DATA_PATH,record)\n",
    "    if label['labels_training_merged'] is None:\n",
    "        label['labels_training_merged'] = [0]*27\n",
    "    records = np.append(records,label['filename'])\n",
    "    label = label['labels_training_merged']\n",
    "    \n",
    "    if label[4] > 0 or label[18] > 0:\n",
    "        label[4] = 1\n",
    "        label[18] = 1\n",
    "    if label[23] > 0 or label[12] > 0:\n",
    "        label[23] = 1\n",
    "        label[12] = 1\n",
    "    if label[26] > 0 or label[13] > 0:\n",
    "        label[26] = 1\n",
    "        label[13] = 1\n",
    "    \n",
    "    pred = load_label(DEBUG_PATH,record)\n",
    "    pred = pred['predicted_label']\n",
    "    \n",
    "    #calc score for each record\n",
    "    label = np.array(label).reshape(1,-1)\n",
    "    pred = np.array(pred).reshape(1,-1)\n",
    "    pred = postprocessing.run(pred)\n",
    "    #label, pred = postprocessing.find_opt_thresold(label, pred)\n",
    "    \n",
    "    scores_competition = np.append(scores_competition,competition_metric.compute(label, pred))\n",
    "    scores_errors = np.append(scores_errors,np.sum(np.abs(label - pred)))\n",
    "    \n",
    "    #add predictions and labels for overall KPI estimation\n",
    "    labels.append(label[0,:])#labels = np.append(labels,label,axis=0)\n",
    "    preds.append(pred[0,:])#preds = np.append(preds,pred,axis=0)\n",
    "\n",
    "    \n",
    "preds = np.array(preds)\n",
    "labels = np.array(labels)\n",
    "#plot modified consustion matrix\n",
    "matrix = competition_metric.compute_modified_confusion_matrix(labels, preds)\n",
    "label_names = ['IAVB'\n",
    "'AF',\n",
    "'AFL',\n",
    "'Brady',\n",
    "'CRBBB',\n",
    "'IRBBB',\n",
    "'LAnFB',\n",
    "'LAD',\n",
    "'LBBB',\n",
    "'LQRSV',\n",
    "'NSIVCB',\n",
    "'PR',\n",
    "'PAC',\n",
    "'PVC',\n",
    "'LPR',\n",
    "'LQT',\n",
    "'QAb',\n",
    "'RAD',\n",
    "'RBBB',\n",
    "'SA',\n",
    "'SB',\n",
    "'SNR',\n",
    "'STach',\n",
    "'SVPB',\n",
    "'TAb',\n",
    "'TInv',\n",
    "'VP'\n",
    " ]\n",
    "\n",
    "\n",
    "plot_confusion_matrix(matrix,label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_competition[np.where(scores_competition == np.inf)] = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.distplot(scores_competition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric for dataset E:  0.35861295589633685\n"
     ]
    }
   ],
   "source": [
    "print(f'Metric for dataset {dataset}: ',competition_metric.compute(labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitriishubin/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in less\n",
      "  \n",
      "/home/dmitriishubin/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in less\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/dmitriishubin/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#keeep records with errors only\n",
    "records = records[np.where(scores_competition < 1)]\n",
    "scores_competition = scores_competition[np.where(scores_competition <1)]\n",
    "scores_competition = 1/scores_competition\n",
    "\n",
    "#TODO: clarify the type of sort\n",
    "score_order = np.argsort(scores_competition)\n",
    "records = records[score_order[::1]]\n",
    "\n",
    "#save a list of files in csv\n",
    "pd.DataFrame(records).to_csv(f'./data/dataset_{dataset}_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    '1st degree av block',\n",
    "'atrial fibrillation',\n",
    "'atrial flutter',\n",
    "'bradycardia',\n",
    "'complete right bundle branch block',\n",
    "'incomplete right bundle branch block',\n",
    "'left anterior fascicular block',\n",
    "'left axis deviation',\n",
    "'left bundle branch block',\n",
    "'low qrs voltages',\n",
    "'nonspecific intraventricular conduction disorder',\n",
    "'pacing rhythm',\n",
    "'premature atrial contraction',\n",
    "'premature ventricular contractions',\n",
    "'prolonged pr interval',\n",
    "'prolonged qt interval',\n",
    "'qwave abnormal',\n",
    "'right axis deviation',\n",
    "'right bundle branch block',\n",
    "'sinus arrhythmia',\n",
    "'sinus bradycardia',\n",
    "'sinus rhythm',\n",
    "'sinus tachycardia',\n",
    "'supraventricular premature beats',\n",
    "    't wave abnormal',\n",
    "    't wave inversion',\n",
    "    'ventricular premature beats',\n",
    "]\n",
    "names = np.array(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR: 84.95073510553385\n",
      "Predictions:  [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      "  1. 0. 0.]]\n",
      "Predictions names:  ['left axis deviation' 'nonspecific intraventricular conduction disorder'\n",
      " 'qwave abnormal' 'sinus arrhythmia' 'sinus rhythm' 't wave abnormal']\n",
      "Label:        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "Name:        HR09822\n"
     ]
    }
   ],
   "source": [
    "def plot_record(record):\n",
    "    \n",
    "    ecg = np.load(DATA_PATH+record+'.npy')\n",
    "    #ecg = preprocessing.run(ecg)\n",
    "    meta = json.load(open(DATA_PATH+ f'{record}.json'))\n",
    "    \n",
    "    pred = load_label(DEBUG_PATH,record)\n",
    "    pred = pred['predicted_label']\n",
    "    \n",
    "    #heatmap = np.array(data[record]['heatmap'],dtype=np.float)\n",
    "    \n",
    "    #plot the data\n",
    "    fig,ax = plt.subplots(figsize=(20,20))\n",
    "    fig.suptitle(record+', labels: '+str(meta['labels_full']))\n",
    "    for i in range(12):\n",
    "        ax.plot(ecg[:,i]+2000*i)\n",
    "    plt.show()\n",
    "    \n",
    "    pred = np.array(pred).reshape(1,-1)\n",
    "    pred = postprocessing.run(pred)\n",
    "    \n",
    "    print('HR:',meta['hr'])\n",
    "    print('Predictions: ',pred)\n",
    "    print('Predictions names: ',names[np.where(pred==1)[1]])\n",
    "    print('Label:       ',meta['labels_training_merged'])\n",
    "    print('Name:       ',meta['filename'])\n",
    "    return np.array(pred)\n",
    "\n",
    "record = records[2]\n",
    "\n",
    "pred = plot_record(record=record)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate a competition score across all datasets + joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_PATH = './data/CV_debug/'\n",
    "DATASETS = ['A','B','D','E','F']\n",
    "\n",
    "list_records = []\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    list_records += [i[:-5] for i in os.listdir(f'./data/CV_debug/{dataset}/') if i.find('.json')!=-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "labels = []\n",
    "\n",
    "preds_A = []\n",
    "labels_A = []\n",
    "\n",
    "preds_B = []\n",
    "labels_B = []\n",
    "\n",
    "preds_D = []\n",
    "labels_D = []\n",
    "\n",
    "preds_E = []\n",
    "labels_E = []\n",
    "\n",
    "preds_F = []\n",
    "labels_F = []\n",
    "\n",
    "for record in list_records:\n",
    "    \n",
    "    if record[0] == 'A':\n",
    "        dataset = 'A'\n",
    "        #print('A')\n",
    "\n",
    "    elif record[0] == 'Q':\n",
    "        dataset = 'B'\n",
    "        #print('B')\n",
    "\n",
    "    elif record[0] == 'I':\n",
    "        dataset = 'C'\n",
    "        #print('C')\n",
    "\n",
    "    elif record[0] == 'S':\n",
    "        dataset = 'D'\n",
    "        #print('D')\n",
    "\n",
    "    elif record[0] == 'H':\n",
    "        dataset = 'E'\n",
    "        #print('E')\n",
    "\n",
    "    elif record[0] == 'E':\n",
    "        dataset = 'F'\n",
    "        #print('F')\n",
    "    \n",
    "    pred_folder = f'./data/CV_debug/{dataset}/'\n",
    "    data_folder = f'./data/{dataset}/formatted/'\n",
    "    \n",
    "    \n",
    "    label = load_label(data_folder,record)\n",
    "    \n",
    "    if label['labels_training_merged'] is None:\n",
    "        label['labels_training_merged'] = [0]*27\n",
    "        \n",
    "    label = label['labels_training_merged']\n",
    "    if label[4] > 0 or label[18] > 0:\n",
    "        label[4] = 1\n",
    "        label[18] = 1\n",
    "    if label[23] > 0 or label[12] > 0:\n",
    "        label[23] = 1\n",
    "        label[12] = 1\n",
    "    if label[26] > 0 or label[13] > 0:\n",
    "        label[26] = 1\n",
    "        label[13] = 1\n",
    "    \n",
    "    label = np.array(label).reshape(1,-1)\n",
    "    \n",
    "    pred = load_label(pred_folder,record)\n",
    "    pred = pred['predicted_label']\n",
    "    pred = np.array(pred).reshape(1,-1)\n",
    "#     pred[np.where(pred >= 0.1)] = 1\n",
    "#     pred[np.where(pred < 0.1)] = 0\n",
    "    pred = postprocessing.run(pred)\n",
    "    #label, pred = competition_metric.find_opt_thresold(np.array(label).reshape(1,-1), np.array(pred).reshape(1,-1))\n",
    "    \n",
    "    preds.append(pred)\n",
    "    labels.append(label)\n",
    "    \n",
    "    if dataset == 'A':\n",
    "        preds_A.append(pred)\n",
    "        labels_A.append(label)\n",
    "    if dataset == 'B':\n",
    "        preds_B.append(pred)\n",
    "        labels_B.append(label)\n",
    "    if dataset == 'D':\n",
    "        preds_D.append(pred)\n",
    "        labels_D.append(label)\n",
    "    if dataset == 'E':\n",
    "        preds_E.append(pred)\n",
    "        labels_E.append(label)\n",
    "    if dataset == 'F':\n",
    "        preds_F.append(pred)\n",
    "        labels_F.append(label)\n",
    "    \n",
    "preds = np.array(preds).reshape(-1,27)\n",
    "labels = np.array(labels).reshape(-1,27)\n",
    "\n",
    "preds_A = np.array(preds_A).reshape(-1,27)\n",
    "labels_A = np.array(labels_A).reshape(-1,27)\n",
    "\n",
    "preds_B = np.array(preds_B).reshape(-1,27)\n",
    "labels_B = np.array(labels_B).reshape(-1,27)\n",
    "\n",
    "preds_D = np.array(preds_D).reshape(-1,27)\n",
    "labels_D = np.array(labels_D).reshape(-1,27)\n",
    "\n",
    "preds_E = np.array(preds_E).reshape(-1,27)\n",
    "labels_E = np.array(labels_E).reshape(-1,27)\n",
    "\n",
    "preds_F = np.array(preds_F).reshape(-1,27)\n",
    "labels_F = np.array(labels_F).reshape(-1,27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score across all datasets:  -1.406513252829043\n",
      "Score, dataset A:  0.5030978715106587\n",
      "Score, dataset B:  0.4566783584916467\n",
      "Score, dataset D:  -1.406513252829043\n",
      "Score, dataset E:  0.336175734965045\n",
      "Score, dataset F:  0.336175734965045\n"
     ]
    }
   ],
   "source": [
    "print('Score across all datasets: ',competition_metric.compute(labels, preds))\n",
    "print('Score, dataset A: ',competition_metric.compute(labels_A, preds_A))\n",
    "print('Score, dataset B: ',competition_metric.compute(labels_B, preds_B))\n",
    "print('Score, dataset D: ',competition_metric.compute(labels_D, preds_D))\n",
    "print('Score, dataset E: ',competition_metric.compute(labels_E, preds_E))\n",
    "print('Score, dataset F: ',competition_metric.compute(labels_E, preds_E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
