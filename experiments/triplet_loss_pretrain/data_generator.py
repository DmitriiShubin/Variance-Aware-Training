# basic libs
import numpy as np
import json
from scipy import signal

# pytorch
import torch
from torch.utils.data import Dataset

# custom modules
np.random.seed(42)


class Dataset_train(Dataset):
    def __init__(self, volums_list, aug, n_classes):

        self.n_classes = n_classes
        self.volums_list = volums_list
        self.preprocessing = Preprocessing(aug)

    def __len__(self):
        return len(self.volums_list)

    def __getitem__(self, idx):

        X, y, X_s, y_s = self.load_data(idx)

        X = torch.tensor(X, dtype=torch.float)
        y = torch.tensor(y, dtype=torch.float)

        X_s = torch.tensor(X_s, dtype=torch.float)
        y_s = torch.tensor(y_s, dtype=torch.float)

        return X, y, X_s, y_s

    def load_data(self, id):

        X = np.load(self.volums_list[id]).astype(np.float32)
        y = json.load(open(self.volums_list[id][:-4] + '.json'))
        y = y['label'][0]

        X = self.preprocessing.run(X=X, zero_label=not bool(y))

        y = np.eye(self.n_classes)[y]

        # second head
        sampled_patient = np.round(np.random.uniform(size=1)[0], 1)
        current_patient = self.patients[id]
        if sampled_patient >= 0.5:
            # NOT the same patient
            records_subset = np.array(self.records_list)[np.where(self.patients != current_patient)]
            if records_subset.shape[0] == 0:
                X_s = X.copy()
                y_s = [-1000]
            else:
                X_s = np.load(DATA_PATH + np.random.choice(records_subset))
                y_s = np.array([0.0])
                X_s = self.preprocessing.run(X=X_s)
        else:
            # the same patient
            records_subset = np.array(self.records_list)[np.where(self.patients == current_patient)]
            records_subset = records_subset[records_subset != self.records_list[id]]
            if records_subset.shape[0] == 0:
                X_s = X.copy()
                y_s = [-1000]
            else:
                X_s = np.load(DATA_PATH + np.random.choice(records_subset))
                y_s = np.array([1.0])
                X_s = self.preprocessing.run(X=X_s)

        return X, y, X_s, y_s


class Preprocessing:
    def __init__(self, aug):

        self.aug = aug
        self.augmentations = Augmentations()

        # 0.75Hz Hamming, HPF, 301th order, Fs = 125Hz
        self.weights_LPF = torch.Tensor(
            [
                9.977585876226865e-05,
                0.00010571799516678048,
                0.00011188275010218841,
                0.00011830373521494838,
                0.0001250129762774059,
                0.00013204067567246377,
                0.00013941497755615728,
                0.00014716173655551012,
                0.00015530429085522095,
                0.0001638632405219749,
                0.00017285623190591796,
                0.00018229774894840627,
                0.00019219891221044583,
                0.00020256728642081232,
                0.0002134066973224658,
                0.0002247170585750871,
                0.00023649420944661236,
                0.0002487297640002363,
                0.00026141097245355535,
                0.00027452059535578246,
                0.0002880367911941006,
                0.00030193301800499956,
                0.00031617794952760705,
                0.0003307354063967884,
                0.0003455643028305168,
                0.00036061860922370647,
                0.0003758473310140643,
                0.0003911945041397167,
                0.0004065992073587618,
                0.0004219955916532342,
                0.00043731292688733883,
                0.00045247566584008964,
                0.0004674035256785249,
                0.0004820115868866668,
                0.0004962104096093368,
                0.0005099061673190026,
                0.0005230007976575119,
                0.0005353921702521461,
                0.0005469742712505667,
                0.000557637404267156,
                0.0005672684073781116,
                0.0005757508857527684,
                0.000582965459455009,
                0.0005887900258998157,
                0.0005931000363983292,
                0.0005957687861798194,
                0.0005966677172286806,
                0.0005956667332321995,
                0.0005926345258899792,
                0.0005874389117950171,
                0.0005799471790557127,
                0.0005700264427929419,
                0.0005575440086084306,
                0.0005423677430899508,
                0.0005243664503870094,
                0.0005034102538651581,
                0.0004793709818184495,
                0.00045212255620138514,
                0.00042154138331895004,
                0.00038750674539954474,
                0.00034990119195863313,
                0.0003086109298556922,
                0.00026352621093386183,
                0.00021454171613252633,
                0.00016155693495780314,
                0.00010447653920335661,
                4.321074981207962e-05,
                -2.2324304214166958e-05,
                -9.220623594411106e-05,
                -0.00016650606075479583,
                -0.00024528786773001806,
                -0.0003286085032478995,
                -0.00041651726776688677,
                -0.0005090556267855364,
                -0.0006062569369270959,
                -0.0007081461880613435,
                -0.0008147397623457673,
                -0.0009260452110225651,
                -0.001042061049774004,
                -0.0011627765733874253,
                -0.0012881716904416187,
                -0.0014182167786713977,
                -0.0015528725616220378,
                -0.0016920900071474376,
                -0.0018358102482554746,
                -0.001983964526744016,
                -0.0021364741600162895,
                -0.0022932505314007287,
                -0.0024541951042453586,
                -0.002619199459988416,
                -0.002788145360350526,
                -0.0029609048337239704,
                -0.003137340285777948,
                -0.0033173046342249383,
                -0.0035006414676389515,
                -0.0036871852281416813,
                -0.003876761417717145,
                -0.004069186827842922,
                -0.004264269792069472,
                -0.004461810461110028,
                -0.0046616010999461655,
                -0.004863426406389505,
                -0.005067063850483155,
                -0.0052722840340635865,
                -0.005478851069753532,
                -0.005686522978593213,
                -0.0058950521054715556,
                -0.006104185551460336,
                -0.006313665622114573,
                -0.006523230290743207,
                -0.006732613675624213,
                -0.006941546530082879,
                -0.0071497567443259595,
                -0.007356969857875368,
                -0.007562909581425869,
                -0.0077672983269073395,
                -0.007969857744518048,
                -0.008170309265461105,
                -0.008368374649104485,
                -0.008563776533261893,
                -0.008756238986282932,
                -0.00894548805962548,
                -0.009131252339586018,
                -0.009313263496847761,
                -0.009491256832520804,
                -0.009664971819340973,
                -0.00983415263671064,
                -0.009998548698270413,
                -0.010157915170708917,
                -0.010312013482531494,
                -0.01046061182154026,
                -0.010603485619788234,
                -0.01074041802481477,
                -0.010871200355987138,
                -0.01099563254482049,
                -0.01111352355817513,
                -0.011224691803280578,
                -0.011328965513571102,
                -0.011426183114373862,
                -0.011516193567529373,
                -0.011598856694085034,
                -0.011674043474247948,
                -0.01174163632385084,
                -0.01180152934662875,
                -0.011853628561679071,
                -0.011897852105527169,
                -0.011934130408293846,
                -0.011962406343516028,
                -0.011982635351248109,
                -0.011994785534128657,
                0.9879043061217542,
                -0.011994785534128657,
                -0.011982635351248109,
                -0.011962406343516028,
                -0.011934130408293846,
                -0.011897852105527169,
                -0.011853628561679073,
                -0.01180152934662875,
                -0.01174163632385084,
                -0.011674043474247948,
                -0.011598856694085034,
                -0.011516193567529375,
                -0.011426183114373864,
                -0.011328965513571102,
                -0.011224691803280578,
                -0.011113523558175133,
                -0.010995632544820493,
                -0.01087120035598714,
                -0.01074041802481477,
                -0.010603485619788236,
                -0.01046061182154026,
                -0.010312013482531496,
                -0.01015791517070892,
                -0.009998548698270415,
                -0.009834152636710641,
                -0.009664971819340973,
                -0.009491256832520806,
                -0.009313263496847761,
                -0.00913125233958602,
                -0.008945488059625481,
                -0.008756238986282932,
                -0.008563776533261895,
                -0.008368374649104487,
                -0.008170309265461107,
                -0.00796985774451805,
                -0.007767298326907341,
                -0.007562909581425871,
                -0.0073569698578753685,
                -0.00714975674432596,
                -0.006941546530082879,
                -0.006732613675624214,
                -0.006523230290743208,
                -0.006313665622114575,
                -0.006104185551460337,
                -0.005895052105471556,
                -0.005686522978593213,
                -0.005478851069753533,
                -0.0052722840340635865,
                -0.005067063850483157,
                -0.004863426406389505,
                -0.004661601099946166,
                -0.00446181046111003,
                -0.004264269792069473,
                -0.004069186827842924,
                -0.003876761417717146,
                -0.0036871852281416826,
                -0.0035006414676389515,
                -0.0033173046342249396,
                -0.003137340285777948,
                -0.0029609048337239713,
                -0.0027881453603505264,
                -0.002619199459988417,
                -0.0024541951042453586,
                -0.002293250531400729,
                -0.0021364741600162908,
                -0.0019839645267440164,
                -0.0018358102482554756,
                -0.001692090007147438,
                -0.0015528725616220389,
                -0.0014182167786713981,
                -0.0012881716904416193,
                -0.0011627765733874256,
                -0.0010420610497740047,
                -0.0009260452110225652,
                -0.0008147397623457679,
                -0.0007081461880613437,
                -0.0006062569369270961,
                -0.0005090556267855364,
                -0.0004165172677668868,
                -0.0003286085032478997,
                -0.00024528786773001817,
                -0.00016650606075479596,
                -9.22062359441111e-05,
                -2.2324304214166975e-05,
                4.3210749812079635e-05,
                0.00010447653920335667,
                0.0001615569349578032,
                0.0002145417161325265,
                0.00026352621093386194,
                0.0003086109298556924,
                0.00034990119195863313,
                0.0003875067453995449,
                0.0004215413833189505,
                0.0004521225562013854,
                0.00047937098181844996,
                0.0005034102538651582,
                0.0005243664503870099,
                0.0005423677430899508,
                0.0005575440086084313,
                0.0005700264427929419,
                0.0005799471790557134,
                0.0005874389117950171,
                0.0005926345258899799,
                0.0005956667332321995,
                0.0005966677172286809,
                0.0005957687861798194,
                0.0005931000363983295,
                0.0005887900258998163,
                0.0005829654594550094,
                0.0005757508857527692,
                0.000567268407378112,
                0.0005576374042671569,
                0.0005469742712505671,
                0.000535392170252147,
                0.0005230007976575119,
                0.0005099061673190033,
                0.0004962104096093368,
                0.00048201158688666705,
                0.0004674035256785249,
                0.0004524756658400901,
                0.00043731292688733883,
                0.00042199559165323464,
                0.0004065992073587622,
                0.00039119450413971704,
                0.00037584733101406485,
                0.00036061860922370663,
                0.0003455643028305172,
                0.0003307354063967887,
                0.0003161779495276074,
                0.00030193301800499956,
                0.0002880367911941008,
                0.00027452059535578246,
                0.00026141097245355557,
                0.0002487297640002363,
                0.0002364942094466125,
                0.00022471705857508734,
                0.0002134066973224659,
                0.00020256728642081243,
                0.00019219891221044594,
                0.0001822977489484065,
                0.00017285623190591796,
                0.00016386324052197513,
                0.00015530429085522095,
                0.00014716173655551012,
                0.00013941497755615728,
                0.00013204067567246377,
                0.0001250129762774059,
                0.00011830373521494838,
                0.00011188275010218841,
                0.00010571799516678048,
                9.977585876226865e-05,
            ]
        )
        self.weights_LPF = self.weights_LPF.view(1, 1, self.weights_LPF.shape[0]).float()
        self.padding_LPF = int((self.weights_LPF.shape[2] - 1) / 2)
        self.padding_LPF = torch.Tensor(np.zeros((self.padding_LPF))).float()

    def run(self, X, zero_label=False):

        # reshape X
        X = np.reshape(X, (1, -1))

        if np.max(X) - np.mean(X) > 10000:
            X = np.clip(X, a_max=np.mean(X) + 300, a_min=0)

        # apply scaling
        if np.std(X) > 0:
            X = (X - np.mean(X)) / np.std(X)
        else:
            X = X - np.mean(X)

        X = X.reshape(-1, 1)

        if self.aug:
            X = self.augmentations.run(X, zero_label)

        return X

    def FIR_filt(self, input, weight, padding_vector):
        input = torch.tensor(input).float()
        input = torch.cat((input, padding_vector), 0)
        input = torch.cat((padding_vector, input), 0)
        input = input.view(1, 1, input.shape[0])
        output = torch.conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)
        output = output.view(output.shape[2])
        output = output.detach().numpy()
        output = np.reshape(output, (-1))
        return output


class Augmentations:
    def __init__(self, prob=0.5):

        self.prob = 1 - prob

    def run(self, X, zero_label):

        if zero_label:
            X = self.make_zeros(X)

        X = self.amplitude_adjusting(X)
        X = self.gaussian_noise(X)

        return X

    def make_zeros(self, X):
        sample = np.random.uniform()
        if sample > self.prob:
            return np.zeros(X.shape)
        else:
            return X

    def gaussian_noise(self, X, std=0.05):

        sample = np.random.uniform()

        if sample > self.prob:

            noise = np.random.normal(loc=0, scale=sample * std, size=X.shape[0])
            noise = noise.reshape(-1, 1)
            X = X + noise

        return X

    def resampling(self, X, stretching_coef=0.05):

        sample = np.random.uniform()
        x_shape = X.shape[0]

        if sample > self.prob:
            X = signal.resample(X, num=int(X.shape[0] * (1 + 2 * (sample - 0.5) * stretching_coef)))

            if X.shape[0] >= x_shape:
                X = X[:x_shape, :]
            else:
                padding = np.zeros((x_shape - X.shape[0], 1))
                X = np.concatenate([X, padding], axis=0)

        return X

    def random_spike(self, X):

        sample = np.random.uniform()

        if sample > self.prob:
            sample = np.random.choice(X.shape[0])
            X[sample] = 121.45

        return X

    def amplitude_adjusting(self, X, std=0.2):

        sample = np.random.uniform()

        if sample > self.prob:
            amp = np.random.uniform(1 - std, 1 + std)
            amp = abs(amp)
            X *= amp

        return X
